#!/usr/bin/env python

import pickle
import numpy
import sys
import os
import getopt
from scipy.sparse import csr_matrix, coo_matrix, hstack
#from svmlight_loader import load_svmlight_file
from sklearn import ensemble
from sklearn.datasets import load_svmlight_file

def resize_csr_matrix(matrix, fc):
    new_feature_count = fc

    n_objects = matrix.shape[0]
    n_features = matrix.shape[1]

    matrix = coo_matrix(matrix)
    n_items = matrix.row.shape[0]

    matrix.row.resize(n_items + n_objects)
    matrix.col.resize(n_items + n_objects)
    matrix.data.resize(n_items + n_objects)

    for i in range(n_objects):
        matrix.row.put(n_items + i, i)
        matrix.col.put(n_items + i, new_feature_count-1)
        matrix.data.put(n_items + i, 1.0)

    return csr_matrix(coo_matrix((matrix.data, (matrix.row, matrix.col)), shape=(n_objects, new_feature_count)))

def usage():
    print '''scikit-train builds the model using sklearn.ensemble.RandomForestClassifier
usage: scikit_rf-train [OPTIONS] <dataset> <model>
OPTIONS:
   Most options below directly correcspond to the arguments of
   sklearn.ensemble.RandomForestClassifier constructor. So, for
   detailed information see scikit learn's documentation.

   So, for detailed information
   -h   --help                     display this screen
        --estimators <num>         the number of estimator (default: 10)
        --criterion <criterion>    function to measure the quality of a split
                                   (default: gini)
        --max_features <features>  the number of features to consider
                                   when looking for the best split
        --max_depth <depth>        the maximum depth of the tree
        --min_samples_split <num>  the minimum number of samples required
                                   to split an internal node
        --min_samples_leaf <num>   the minimum number of samples required
                                   to split an internal node
        --max_leaf_nodes <num>     grow trees with max_leaf_nodes in
                                   best-first fashion. Best nodes are defined
                                   as relative reduction in impurity.
                                   If None then unlimited number of leaf nodes
                                   (default: None)
        --min_impurity_split <t>   Threshold for early stopping in tree growth.
                                   A node will split if its impurity is above
                                   the threshold, otherwise it is a leaf
                                   (default: 1e-7)
        --bootstrap <flag>         Whether bootstrap samples are used
                                   when building trees (default: True)
        --oob_score <flag>         Whether to use out-of-bag samples to
                                   estimate the generalization accuracy
                                   (default: False)
        --jobs <num>             The number of jobs to run in parallel
                                   for both fit and predict. If -1, then the
                                   number of jobs is set to the number
                                   of cores (default: 1)
        --random_state <seed>      random seed (default: None)
        --verbose <num>            controls the verbosity of the tree building
                                   process (default: 0)
        --warm_start <flag>        When set to True, reuse the solution of
                                   the previous call to fit and add more
                                   estimators to the ensemble, otherwise, just
                                   fit a whole new forest (default: False)
        --class_weight <weights>   ignored (default: None)
'''

def coerce(x):
    try:
        return int(x)
    except:
        return float(x)

def str2bool(v):
  return v.lower() in ("yes", "true", "y", "t", "1")

# defaults
estimators = 10
criterion='gini'
max_features='auto'
max_depth=None
min_samples_split=2
min_samples_leaf=1
min_weight_fraction_leaf=0
max_leaf_nodes=None
min_impurity_split=1e-7
bootstrap=True
oob_score=False
jobs=1
random_state = None
verbose=0
warm_start=False
class_weight=None

#
opts,args = getopt.getopt(sys.argv[1:], 'h',
                          ['help', 'estimators=', "criterion=",
                           'max_features=', 'max_depth=', 'min_samples_split=',
                           'min_samples_leaf=', 'min_weight_fraction_leaf=',
                           'max_leaf_nodes=', 'min_impurity_split=',
                           'bootstrap=', 'oob_score=', 'jobs=', 'random_state=',
                           'verbose=', 'warm_start=', 'class_weight=']
)

for o,a in opts:
    if o in ("-h", "--help", ):
        usage()
        sys.exit()
    if o in ("--estimators", ):
        estimators = int(a)
    if o in ("--criterion", ):
        criterion = a
    if o in ("--max_features", ):
        max_features = a
        if a in ('auto', 'sqrt', 'log2'):
            pass
        elif a == 'None':
            max_features = None
        else:
            max_features = coerce(a)
    if o in ("--max_depth", ):
        max_depth = a
        if a == 'None':
            max_depth = None
        else:
            max_depth = int(a)
    if o in ("--min_samples_split", ):
        min_samples_split = coerce(a)
    if o in ("--min_samples_leaf", ):
        min_samples_leaf = coerce(a)
    if o in ("--min_weight_fraction_leaf", ):
        min_weight_fraction_leaf = float(a)
    if o in ("--max_leaf_nodes", ):
        if a == 'None':
            max_leaf_nodes = None
        else:
            max_leaf_nodes = int(a)
    if o in ("--min_impurity_split", ):
        min_impurity_split = float(a)
    if o in ("--bootstrap", ):
        bootstrap = str2bool(a)
    if o in ("--oob_score", ):
        oob_score = str2bool(a)
    if o in ("--jobs", ):
        jobs = int(a)
    if o in ("--random_state", ):
        if a == 'None':
            random_state = None
        else:
            random_state = int(a)
    if o in ("--verbose", ):
        verbose = int(a)
    if o in ("--warm_start", ):
        warm_start = str2bool(a)

X_train, y_train = load_svmlight_file(args[0])

if 'HERISVM_FC' in os.environ:
    fc=int(os.environ['HERISVM_FC'])
    X_train = resize_csr_matrix(X_train, fc)

clf = ensemble.RandomForestClassifier(
    n_estimators=estimators, criterion=criterion, max_features=max_features,
    max_depth=max_depth, min_samples_split=min_samples_split,
    min_samples_leaf=min_samples_leaf,
    min_weight_fraction_leaf=min_weight_fraction_leaf,
    max_leaf_nodes=max_leaf_nodes, #min_impurity_split=min_impurity_split,
    bootstrap=bootstrap, oob_score=oob_score, n_jobs=jobs, random_state=random_state,
    verbose=verbose, warm_start=warm_start, class_weight=class_weight)
clf.fit(X_train, y_train)
with open(args[1], 'wb') as f:
    pickle.dump(clf, f)
